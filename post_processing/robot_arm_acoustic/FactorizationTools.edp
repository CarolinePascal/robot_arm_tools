load "PETSc"

func real[int] regularizedFactorization(matrix A, real[int] b, real lambda, bool verbose)
{
    string solver;
    if(!HasType("MATSOLVER", "mumps") && !HasType("MATSOLVER", "superlu"))
        exit(0);
    else
        solver = (HasType("MATSOLVER", "mumps") ? "mumps" : "superlu");

    //OBJECTIVE : Solve min_x ||Ax - b||² + \lambda||x||²
    load "ffrandom"
    srandomdev();
    int maxrang = 2^31 - 1;

    int m = A.m;
    real[int] D(m);
    D = 1;
    int[int][int] restriction(1);
    restriction[0].resize(0);
    matrix sp = D;
    Mat H(sp, restriction, D);

    //Minimized function : J(x) = (Ax - b)^T.(Ax - b) + \lambda(x^T.x)
    func real J(real[int]& in) {
        real[int] out = A*in;
        out -= b;
        return out'*out + lambda*(in'*in);
    }

    //Actual minimization target : f(x) = (Ax - b)^T.(Ax - b)
    func real f(real[int] in)
    {
        real[int] out = A*in;
        out -= b;
        return out'*out;
    }

    //Gradient of J(x) = 2A^T.A.x - 2A^T.b + 2\lambdax
    func real[int] DJ(real[int]& in) {
        matrix tmp = A'*A;
        real[int] out = tmp*in;
        out *= 2;
        
        real[int] tmp2 = A'*b;
        out -= 2*tmp2;

        out += 2*lambda*in;
        return out;
    }

    //Hessian of J(x) = 2A^T.A + 2\lambdaI
    func int funcH(real[int]& in) {
        matrix out = A'*A;
        out = 2*out;

        real[int] diag(A.m);
        diag = 2*lambda;
        matrix tmp = diag;
        out += tmp;
        
        H = out;
        return 0;
    }

    //WORKING : Solve with TAO
    //Solve
    real[int] x(m);
    for(int i = 0; i < m; i++)
    {
        x[i] = random()/maxrang;
    }

    if(mpirank == 0 && verbose)
    {
        cout << "J([" ;
        for(int i = 0; i < x.n-1 ; i++)
        {
            cout << x[i] << ", ";
        }
        cout << x[x.n-1] << "]) = " << J(x) << endl;
    }

    //Unconstrained minimization
    // - Newton line search (BNLS)
    // - Newton trust region (BNTR)
    // - Newton trust region with line search (BNTL)

    //Least-squares => Unsupported

    //Quadratic solvers => Assumes quadratic form of the objective function
    // - Gradient Projection Conjugate Gradient Method (GPCG)
    // - Interior-Point Newton’s Method (BQPIP)

    //Remark : All solvers assume that the Hessian in known !

    //TaoSolve(H, J, DJ, x, sparams = "-tao_monitor -tao_type bnls -pc_type lu -pc_factor_mat_solver_type " + solver, HessianRoutine = funcH);

    TaoSolve(H, J, DJ, x, sparams = "-tao_monitor -tao_type gpcg -pc_type lu -pc_factor_mat_solver_type " + solver, HessianRoutine = funcH);

    if(mpirank == 0 && verbose)
    {
        cout << "J([" ;
        for(int i = 0; i < x.n-1 ; i++)
        {
            cout << x[i] << ", ";
        }
        cout << x[x.n-1] << "]) = " << J(x) << endl;

        real[int] output = A*x;
        cout << "A*x = " << output << endl;
        cout << "b = " << b << endl;
        cout << "||Ax - b||^2 = " << f(x) << endl;
        cout << "||x||^2 = " << x'*x << endl;
    }

    return x;
}